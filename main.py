import keras
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from keras.applications.vgg19 import VGG19
from keras.models import Model, Sequential
from keras.layers import Dense, Dropout, Flatten, Convolution2D, Activation, MaxPooling2D

df_train = pd.read_csv('data2/labels.csv')
df_test = pd.read_csv('data2/sample_submission.csv')

targets_series = pd.Series(df_train['breed'])
one_hot = pd.get_dummies(targets_series, sparse=True)

one_hot_labels = np.asarray(one_hot)

im_size = 48
epochs = 6
batch_size = 64

x_train = []
y_train = []
x_test = []

i = 0
for f, breed in tqdm(df_train.values):
    img = cv2.imread('data2/train/{}.jpg'.format(f))
    label = one_hot_labels[i]
    x_train.append(cv2.resize(img, (im_size, im_size)))
    y_train.append(label)
    i += 1

for f in tqdm(df_test['id'].values):
    img = cv2.imread('data2/test/{}.jpg'.format(f))
    x_test.append(cv2.resize(img, (im_size, im_size)))

y_train_raw = np.array(y_train, np.uint8)
x_train_raw = np.array(x_train, np.float32) / 255.
x_test = np.array(x_test, np.float32) / 255.

print(x_train_raw.shape)
print(y_train_raw.shape)
print(x_test.shape)

num_class = y_train_raw.shape[1]

X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)

# Create the base pre-trained model
# Can't download weights in the kernel
base_model = VGG19(  # weights='imagenet',
    weights=None, include_top=False, input_shape=(im_size, im_size, 3))

# Add a new top layer
x = base_model.output
x = Flatten()(x)
predictions = Dense(num_class, activation='softmax')(x)

# This is the model we will train
# model = Model(inputs=base_model.input, outputs=predictions)
#
# # First: train only the top layers (which were randomly initialized)
# for layer in base_model.layers:
#     layer.trainable = False


model = Sequential()
model.add(Convolution2D(32, 3, 3, input_shape=(im_size, im_size, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_class))
model.add(Activation('sigmoid'))


model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]
# print(model.summary())

# load pre-trained weights
model.load_weights('models/basic_cnn.h5')
print('model loaded')

history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), epochs=epochs, batch_size=batch_size
                    , verbose=1)

model.save_weights('models/basic_cnn.h5')
print('model saved')

preds = model.predict(x_test, batch_size=batch_size, verbose=1)

submission = pd.DataFrame(preds)
# Set column names to those generated by the one-hot encoding earlier
col_names = one_hot.columns.values
submission.columns = col_names
# Insert the column id from the sample_submission at the start of the data frame
submission.insert(0, 'id', df_test['id'])

submission.to_csv('submissions/submission.csv', encoding='utf-8', index=False)


print(history.history.keys())
plt.figure(1)

# summarize history for accuracy
plt.subplot(211)
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')

# summarize history for loss
plt.subplot(212)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
